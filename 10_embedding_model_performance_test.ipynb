{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw kaggle data into datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = pd.read_csv('./inputdata/Mental_Health_FAQ.csv')\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to look at some of the data, run this cell a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A personality disorder is a pattern of thoughts, feelings, and behaviours that last for a long time and causes some sort of problem or distress. \n",
      " Schizoid personality disorder or SPD affects social interactions and relationships. People with SPD may have a hard time relating to others and showing emotions. They may avoid close relationships and prefer to spend their time alone, seeming distant even to close family members. Many people don’t respond to strong emotions like anger, even when others try to provoke them. On the outside, people with SPD may seem cold or aloof, showing little emotion. \n",
      " While they have a similar name, schizoid personality disorder isn’t the same as schizophrenia. \n",
      " Schizoid personality disorder is believed to be relatively uncommon. While some people with SPD may see it as part of who they are, other people may feel a lot of distress, especially around social interactions. Some medications may help people manage symptoms and psychotherapy may help people build new skills and improve relationships. \n",
      " To find help for schizoid personality disorder, talk to your family doctor, find a psychologist through the BC Psychological Association, or call 811 to talk to a HealthLink BC navigator.\n"
     ]
    }
   ],
   "source": [
    "# select the Answers column for a random row to get a flavor of the data\n",
    "answer = raw_data['Answers'].sample(1).values[0]\n",
    "print(answer)\n",
    "\n",
    "# if you want to see an example of a long answer, uncomment this\n",
    "# answer = raw_data.loc[raw_data['Question_ID'] == 7535002, 'Answers'].iloc[0]\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model to run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model e5-base-v2\n",
      "Embedding on full answers\n"
     ]
    }
   ],
   "source": [
    "model_list = [\"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", \"instructor_large\", \"instructor_xl\", \"e5-base-v2\", \"e5-large-v2\"]\n",
    "model_to_use = model_list[4]\n",
    "print(\"Using model \" + model_to_use)\n",
    "\n",
    "\n",
    "# create the output folder name\n",
    "dbf_postscript = \"_full_answers\"\n",
    "print(\"Embedding on full answers\")\n",
    "data_frame_to_use = raw_data\n",
    "dbf = \".\\db_\" + model_to_use + dbf_postscript "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have split the instantiation of the model into two cells which makes it look a little convoluted. I did this because loading big models takes time. I was also continually making changes to my `create_vector_db.py` file and I did not want to have to reload the big model every time I made a change to my file. You can obviously combine the two cells and make it more readable but only do that when you are done making changes to `create_vector_db.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a dictionary where the keys are the model names and the values are the functions or actions we want to take for each one.\n",
    "\n",
    "The challenge here is that the actions for each model_to_use value are not consistent - in some cases, you're importing a model \n",
    "and in others, you're setting a value.\n",
    "\n",
    "For the models where you're importing a model and instantiating it with a string, define a function that does that and use \n",
    "the function in your dictionary. For the models where you're just setting a string, you can just put the string in your dictionary.\n",
    "'''\n",
    "\n",
    "def import_instructor_model(model_name):\n",
    "    from InstructorEmbedding import INSTRUCTOR\n",
    "    return INSTRUCTOR(model_name)\n",
    "\n",
    "def import_sentence_transformer(model_name):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "def import_openai_model(model_name):\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return model_name\n",
    "\n",
    "model_to_function_map = {\n",
    "    \"instructor_large\": lambda: import_instructor_model('hkunlp/instructor-large'),\n",
    "    \"instructor_xl\": lambda: import_instructor_model('hkunlp/instructor-xl'),\n",
    "    \"e5-base-v2\": 'intfloat/e5-base-v2',\n",
    "    \"e5-large-v2\": 'intfloat/e5-large-v2',\n",
    "    \"all-MiniLM-L6-v2\": lambda: import_sentence_transformer('sentence-transformers/all-MiniLM-L6-v2'),\n",
    "    \"text-embedding-ada-002\": lambda: import_openai_model('text-embedding-ada-002'),\n",
    "}\n",
    "\n",
    "if model_to_use not in model_to_function_map:\n",
    "    raise ValueError(f\"model_to_use must be one of {', '.join(model_to_function_map.keys())}\")\n",
    "\n",
    "model = model_to_function_map[model_to_use]\n",
    "if callable(model):\n",
    "    model = model()  # Call the function if the model is a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace models have a useful summary of important model parameters. If you paste the model name in a cell you get to see these. For reference here are the outputs from Instructor_Large and all_miniLM_L6_v2\n",
    "\n",
    "```\n",
    "INSTRUCTOR(\n",
    "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
    "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
    "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
    "  (3): Normalize()\n",
    ")\n",
    "```\n",
    "```\n",
    "SentenceTransformer(\n",
    "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
    "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
    "  (2): Normalize()\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 of my convoluted load process, please feel free to combine this with the previous cell code once you are done making changes to `create_vector_db.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_vector_db import InstructorEmbeddingModel, e5EmbeddingModel, AllMiniLML6v2, OpenAIAda\n",
    "from importlib import reload\n",
    "import create_vector_db\n",
    "\n",
    "reload(create_vector_db)\n",
    "\n",
    "model_class_map = {\n",
    "    \"instructor_large\": InstructorEmbeddingModel,\n",
    "    \"instructor_xl\": InstructorEmbeddingModel,\n",
    "    \"e5-base-v2\": e5EmbeddingModel,\n",
    "    \"e5-large-v2\": e5EmbeddingModel,\n",
    "    \"all-MiniLM-L6-v2\": AllMiniLML6v2,\n",
    "    \"text-embedding-ada-002\": OpenAIAda\n",
    "}\n",
    "\n",
    "if model_to_use not in model_class_map:\n",
    "    raise ValueError(f\"model_to_use must be one of {', '.join(model_class_map.keys())}\")\n",
    "\n",
    "my_model = model_class_map[model_to_use](model, dbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the EmbeddingModel class has a method to create embeddings based on other columns, or a combination of columns,\n",
    "# I have only tested this when we use the \"Answers\" column as the source of the embeddings.  \n",
    "\n",
    "name_answer = \"Answers\"\n",
    "#name_combined = \"Combined\"\n",
    "#name_faq = \"Questions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already exists, reading from it\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client = my_model.client\n",
    "    collection_answers = client.get_collection(name=name_answer.lower())\n",
    "    #collection_question = client.get_collection(name=name_faq.lower())\n",
    "    #collection_combined = client.get_collection(name=name_combined.lower())\n",
    "    print(\"Database already exists, reading from it\")\n",
    "\n",
    "except (NameError, ValueError):\n",
    "    print(\"Collection does not exist, creating it\")\n",
    "    my_model.embed_and_save_knowledge_base(text_faq_data_df = data_frame_to_use, embedding_column = name_answer)\n",
    "    client = my_model.client\n",
    "    collection_answers = client.get_collection(name=name_answer.lower())\n",
    "    #collection_question = client.get_collection(name=name_faq.lower())\n",
    "    #collection_combined = client.get_collection(name=name_combined.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File question_embeddings\\e5-base-v2.csv already exists so just loading it\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "question_embedding_file_to_create = Path(\"./question_embeddings\") / f\"{model_to_use}.csv\"\n",
    "\n",
    "if question_embedding_file_to_create.is_file():\n",
    "    print(f\"File {question_embedding_file_to_create} already exists so just loading it\")\n",
    "    questions_df = pd.read_csv(question_embedding_file_to_create)\n",
    "    # Convert the value in row['Embeddings'] from a string to a list\n",
    "    questions_df['Embeddings'] = questions_df['Embeddings'].apply(ast.literal_eval)\n",
    "else:\n",
    "    questions_df = pd.DataFrame(raw_data['Questions'])\n",
    "    questions_df = my_model.embed_test_questions(questions_df, 'Questions')\n",
    "    questions_df.to_csv(question_embedding_file_to_create, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how to perform a single lookup of the database.   \n",
    "Note that the \"Scores\" column in the dataframe is :  `1 - (cosine_similarity(embedded of question, retrieved embedding vector from db))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Questions   \n",
      "0        What does it mean to have a mental illness?  \\\n",
      "1  What's the difference between mental health an...   \n",
      "2                    Who does mental illness affect?   \n",
      "3                        What causes mental illness?   \n",
      "\n",
      "                                             Answers    Scores  \n",
      "0  Mental illnesses are health conditions that di...  0.130379  \n",
      "1  ‘Mental health’ and ‘mental illness’ are incre...  0.140432  \n",
      "2  It is estimated that mental illness affects 1 ...  0.140581  \n",
      "3  It is estimated that mental illness affects 1 ...  0.141543  \n",
      "Match found\n"
     ]
    }
   ],
   "source": [
    "#question = \"What is depression?\" # not in FAQ so expect \"No match found\"\n",
    "#question = \"Does psilocybin help with depression?\" # not in FAQ so expect \"No match found\"\n",
    "#question = 'How can I find a mental health professional for myself or my child?' # from FAQ so expect \"Match found\"\n",
    "question = \"What does it mean to have a mental illness?\" # from FAQ so expect \"Match found\"\n",
    "\n",
    "search_results = my_model.search_collection_using_text(collection=collection_answers, text=question, num_docs=4)\n",
    "print(search_results)\n",
    "\n",
    "if question in search_results['Questions'].values:\n",
    "    print(\"Match found\")\n",
    "else:\n",
    "    print(\"No match found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test which iterates through all 98 questions and checks if the top search result matches the expected answer. We also check to see if the top 4 search results contains the expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 98\n",
      "Top Matched questions: 70\n",
      "Matched questions: 93\n"
     ]
    }
   ],
   "source": [
    "def check_matches(row):\n",
    "    embedding = row['Embeddings']\n",
    "    search_results = my_model.search_collection_using_embedding(collection_answers, embedding=embedding, num_docs=4)\n",
    "    \n",
    "    # Check if current question is in search results\n",
    "    match = row['Questions'] in search_results['Questions'].values\n",
    "\n",
    "    # Check if current question is the top match in search results. The top match has the highest cosine similarity or lowest Score\n",
    "    # I could equally have rerun the search with num_docs=1 and checked if the expected answer was the only result\n",
    "    top_match = row['Questions'] == search_results.loc[search_results['Scores'].idxmin(), 'Questions']\n",
    "\n",
    "    return pd.Series([match, top_match])\n",
    "\n",
    "# Apply check_matches to each row in questions_df and count True values\n",
    "questions_df[['Match', 'Top_Match']] = questions_df.apply(check_matches, axis=1)\n",
    "match_count = questions_df['Match'].sum()\n",
    "top_match_count = questions_df['Top_Match'].sum()\n",
    "\n",
    "print(f\"Total questions: {len(questions_df)}\")\n",
    "print(f\"Top Matched questions: {top_match_count}\")\n",
    "print(f\"Matched questions: {match_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc81e089c2f5279f21356681884ce5bcbbb440b9afbfbfc737ea25b5a34dac96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
